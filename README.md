# ğŸ§© Problems to Solve â€” Then and Now

Modern computation is evolving faster than ever â€” pushing boundaries of physics, architecture, and energy efficiency. From early mechanical machines to CMOS & FinFET breakthroughs, humanity has always pushed toward one goal:

> **Unlock Zetta-scale computing to solve the worldâ€™s most complex scientific and engineering challenges.**


---

## 1ï¸âƒ£ In the Beginningâ€¦ â€” Early Computing History

### Evolution of Foundational Machines

| Machine        | Technology                          | Purpose                                                  | Key Takeaway                                      |
|----------------|--------------------------------------|----------------------------------------------------------|----------------------------------------------------|
| **Bombe**      | Mechanical rotors                    | Break Enigma cryptographic code during WWII              | First problem-specific computation machine         |
| **ENIAC**      | Vacuum tubes; Decimal number system  | Perform complex mathematical & ballistic calculations    | First Turing-complete, general-purpose computer    |
| **EDVAC**      | Vacuum tubes; Binary von Neumann     | Introduced stored-program architecture                   | Foundation of all modern processors                |
| **CMOS ENIAC** | Integrated CMOS technology           | Miniaturized ENIAC into a handheld IC form              | Shows extreme progress in scaling & efficiency     |

### **Evolution Focus:**
**Smaller â†’ Faster â†’ More efficient â†’ More capable**

---

## 2ï¸âƒ£ Next-Generationâ€¦ â€” Modern High-Impact Challenges

Todayâ€™s computing must solve globally critical, computation-heavy missions:

- ğŸŒ **Earthâ€“water system & sustainability modeling**
- ğŸŒ§ **Weather prediction & hyper-accurate forecasting**
- ğŸŒ‹ **Earthquake simulation & disaster mitigation**
- ğŸ”¬ **Atomic-scale modeling & semiconductor innovation**
- ğŸŒŒ **Validating fundamental laws of physics**
- ğŸš„ **Virtual particle accelerators**
- â„ï¸ **Room-temperature superconductivity discovery**
- ğŸ§¬ **Genomics, drug discovery & precision medicine**

These challenges demand:
- **Massive parallelism**
- **Extreme computational performance**
- **New architectures beyond CMOS**
- **Breakthroughs toward Zetta-scale capability**

---

# ğŸ“ˆ 50 Years of Microprocessor Scaling â€” Mooreâ€™s Law and Beyond

For over five decades, semiconductor progress has been guided by **Mooreâ€™s Law**, which states:

> **The number of transistors on a chip doubles approximately every 2 years.**

This exponential scaling drove massive improvements in multiple computing metrics:

- âš™ï¸ **Transistor Count** (orange)
- ğŸš€ **Single-thread Performance** (blue)
- âš¡ **Operating Frequency** (green)
- ğŸ”¥ **Power Consumption** (red)
- ğŸ§  **Number of Cores** (black)

---

## ğŸ” Key Observations from the Trend Data

### **1970s â€“ Early 2000s: Golden Era of Frequency Scaling**
- Frequencies increased steadily.
- Single-thread performance saw continuous, predictable gains.
- Transistor counts rose sharply each generation.

### **Mid-2000s: The Power Wall**
Scaling frequency hit a thermal barrier:
- Power and heat limits prevented further GHz increases.
- Voltage scaling slowed down.
- Frequency saturation forced a paradigm shift.

### **Industry Pivoted To:**
- **Multi-core processors**
- **Parallel computing models**
- **Datacenter-scale architectures**

With the rise of smartphones (e.g., 2007 iPhone), **power efficiency** became more important than raw clock speed.

---

## ğŸŒ Towards Future Scaling: Datacenter Computing Era

Modern computing innovation has expanded from chip-level focus to large-scale system architectures.

### **Scaling Hierarchy**
âœ” **Chip â†’ Package â†’ System â†’ Cluster â†’ Cloud/HPC**

### **Key Principles Today**
- Efficiency matters more than pure transistor count.
- Memory bandwidth, interconnects, and energy efficiency dominate modern performance.
- Power-aware architectures are essential.

---

## ğŸš€ Zetta-Scale Needs Continuous Innovation

Reaching **Zetta-scale computing** in future decades requires reinventing scaling beyond classical Mooreâ€™s Law.

### **New Paths Forward:**
- **3D transistor architectures:** FinFETs, GAAFETs, CFETs  
- **Advanced material & device engineering**  
- **Specialized accelerators** for AI, ML, and scientific workloads  
- **Energy-aware architectures** to manage thermal dissipation  

### **The New Rule of Scaling**
> Scaling is no longer just about shrinking â€”  
> **itâ€™s about smarter design, power efficiency, and architecture-first innovation.**

---

## ğŸ“± Rise of Mobile Computing

The introduction of the iPhone in **2007** fundamentally reshaped the global computing ecosystem. What began as a consumer electronics trend quickly transformed into a new computing paradigm. As shown in market data, **smartphone shipments skyrocketed past PC shipments**, reaching **1.4 billion units per year by 2015**, while global PC shipments stabilized around **300 million units** annually.

This marked a pivotal moment:  
> **The center of gravity of personal computing moved from the desk to the pocket.**

### Why Mobile Became Dominant
Mobile devices rapidly became the primary computing platform due to:
- **Portability & always-on connectivity**  
  Smartphones made information access instant and ubiquitous.
- **Rich sensor ecosystems**  
  Cameras, GPS, accelerometers, gyroscopes, and proximity sensors enabled new classes of applications.
- **App ecosystem explosion**  
  Millions of apps redefined productivity, entertainment, and communication.
- **Affordable computing at scale**  
  Device costs dropped while capabilities increased exponentially.

Smartphones transitioned from â€œcommunication toolsâ€ to **general-purpose supercomputers on battery power**, used globally by billions.

---

## âš  Performance Limitations in Mobile Devices

However, mobile computing brought new engineering constraints that **PCs never had to face**. Smartphones must operate within strict physical limits:

### ğŸš« Thermal Limitations
- No active cooling (fans or airflow)
- Very small heat dissipation area
- High-performance workloads quickly generate heat  
  â†’ leading to **thermal throttling**

### ğŸ”‹ Power Constraints
- Entire device runs on a small Li-ion battery
- Higher frequency = exponential power consumption
- Sustained high-performance workloads drain battery rapidly

### ğŸ“ Form-Factor Restrictions
- Ultra-compact enclosures
- No room for heat pipes, vapor chambers (early devices), or large heatsinks
- Limited silicon area for high-power CPU cores

---

### Consequence:  
### ğŸ“‰ **Frequency Scaling Stalled**

After ~2006, pushing CPUs beyond ~2â€“3 GHz became thermally impractical for smartphones. Instead of increasing frequency, engineers shifted focus:

### âš™ï¸ The New Mobile Strategy
- **Multi-core CPUs** to distribute workload
- **big.LITTLE architectures** (performance + efficiency cores)
- **Specialized accelerators**  
  - GPU for graphics & parallel compute  
  - ISP for image/video  
  - NPU for machine learning  
  - Modem DSPs for wireless communication  
- **Advanced semiconductor process nodes**  
  Reduced leakage, lower power, and improved transistor density

This marked the end of classical **Dennard scaling** and the birth of power-driven architectural innovation.

---

## ğŸš€ Implications for Zetta-Scale Computing

The mobile revolution didnâ€™t just change consumer devices â€”  
**it redefined the trajectory of the entire semiconductor industry.**

### How Mobile Pushed Us Toward Zetta-Scale

#### 1ï¸âƒ£ Efficiency Became the Priority  
Smartphones forced chip designers to pursue **performance per watt**, not raw performance.  
This principle now drives:
- Data centers  
- AI accelerators  
- Supercomputers

#### 2ï¸âƒ£ Heterogeneous Compute Became Standard  
Mobile pioneered architectures combining multiple specialized engines.  
Modern HPC systems now adopt similar models:
- CPUs + GPUs  
- CPUs + AI accelerators  
- Memory-centric compute  
- Chiplet-based modular designs

#### 3ï¸âƒ£ Workload Offloading to Datacenters  
As mobile devices became the main computing platform, heavy computation moved to the cloud:
- AI inference
- Real-time translation
- Recommendation engines
- Large-scale simulations

This increased demand for:
- Hyperscale datacenters  
- High-bandwidth networks  
- Energy-efficient compute nodes

#### 4ï¸âƒ£ Breaking Mooreâ€™s Law Became Urgent  
With smartphone performance hitting thermal walls, new innovations were required:
- Multi-core scaling  
- FinFET â†’ GAAFET transitions  
- 3D stacking, TSVs, chiplets  
- New memory technologies (HBM, MRAM)  
- Hardware-software co-design  

These techniques are essential to achieving:
> **Zetta-scale systems capable of >10Â²Â¹ FLOPS**

---

# ğŸš€ Path to Zetta-Scale Computing

Modern compute performance is commonly measured in **FLOPS** (Floating-Point Operations Per Second). Over the last four decades, High-Performance Computing (HPC) systems have evolved from **Gigaflop** machines that filled an entire room to **Exaflop** supercomputers capable of simulating biological systems or training trillion-parameter AI models.

This exponential growth has shaped breakthroughs in physics, medicine, climate science, national security, and now artificial intelligence.  
However, the rate of improvement is slowing â€” and the next milestone, **Zetta-scale computing**, requires breakthroughs far beyond traditional scaling.

![Path to Zetta-Scale Computing](./images/path_to_zetta.png)

---

## ğŸ“ˆ Evolution of Compute Performance

| Era | Compute Capability | Example System | Year | Power Usage |
|-----|------------------|----------------|------|-------------|
| **GigaFLOPS** | 10â¹ FLOPS | Early workstation systems | ~1984 | ~1 kW |
| **TeraFLOPS** | 10Â¹Â² FLOPS | Cray XT5 Jaguar | 2008 | 7 MW |
| **PetaFLOPS** | 10Â¹âµ FLOPS | Titan (Cray XK6) | 2012 | 9 MW |
| **ExaFLOPS** | 10Â¹â¸ FLOPS | Frontier, USA | 2021 | 29 MW |
| **ZettaFLOPS (Goal)** | 10Â²Â¹ FLOPS | Future HPC | ~2035 | **50â€“100 MW** |

### ğŸ§® Understanding the Scaling Ratio

Each generation represents a **1000Ã— increase** in computational capability. Historically, reaching each milestone has taken about **8â€“14 years**, driven by:

- Advances in semiconductor process nodes  
- Architectural parallelism  
- Domain-specific accelerators  
- Improvements in cooling and power delivery  
- Smarter compiler and runtime software  

But now, **power is scaling faster than performance**, which is unsustainable.

---

## âš¡ Why Power is the #1 Barrier

Zetta-scale performance is achievable in theory, but with current architectures, it becomes **thermally and economically infeasible**.

### If scaled linearly from todayâ€™s systems:
> **ZettaFLOPS would require 50â€“100 MW**  
> *(equivalent to powering an entire small city)*

This creates major challenges:

- ğŸ¥µ Cooling becomes prohibitively expensive  
- ğŸ’¸ Energy cost becomes unsustainable  
- ğŸŒ Environmental impact increases sharply  
- ğŸ”Œ Power delivery infrastructure becomes a bottleneck  

Even todayâ€™s Exascale machines push the limits with ~30 MW consumption.

---

## ğŸ”¥ Computing Needs Are Growing Faster Than We Can Scale

While computing systems slow down in scaling, human demands are accelerating.

### Future workloads driving the need for Zetta-scale systems:

#### ğŸ”¬ Atomic-Level Digital Twins  
Simulating chemistry, materials, and molecules at quantum accuracy requires massive parallelism.

#### ğŸ¤– Artificial General Intelligence (AGI)  
Training & inference for AGI-scale models need orders of magnitude more compute than today's AI.

#### ğŸš— Fully Autonomous Robotics  
Real-time decision-making at global scale for drones, vehicles, and robots.

#### ğŸ§¬ Precision Medicine  
Large-scale biological simulation for personalized treatments.

#### ğŸ›°ï¸ Planetary-Scale Climate & Space Simulation  
Accurate, high-resolution models essential for predicting extreme events and enabling space exploration.

**Simply put: the world's scientific, medical, and AI problems are becoming Zetta-scale problems.**

---

## ğŸ§  What Must Evolve to Reach Zetta Scale?

Achieving Zetta-scale computing requires a **full-stack revolution**, not just faster chips.

### ğŸ”¹ 1. Device-Level Innovation  
- FinFET â†’ GAAFET â†’ CFET â†’ 2D Materials (MoSâ‚‚), CNT-FETs, Spintronics  
- 100Ã— reduction in energy per computation  
- Improved channel control and reduced leakage  
- Lower interconnect resistance and capacitance  

### ğŸ”¹ 2. 3D Stacking & Advanced Packaging  
- Multi-die chiplet designs  
- Logic-on-memory stacking for near-zero data movement  
- High-bandwidth die-to-die interconnects (UCIe, XSR SerDes)  
- Reduced communication energy, which dominates AI/HPC power  

### ğŸ”¹ 3. Heterogeneous Accelerators  
- GPUs, NPUs, and matrix engines  
- Domain-specific silicon for AI/ML workloads  
- Low-precision compute formats (FP8, FP4, INT1/Binary)  

### ğŸ”¹ 4. System-Level Cooling  
Air cooling is no longer viable.

- Liquid immersion cooling  
- Two-phase fluid cooling  
- Cryogenic computing for extreme efficiency  

### ğŸ”¹ 5. Rack-Level â€œSingle Socketâ€ Architecture  
Future supercomputers behave like a **single massive integrated processor**, enabled by:

- Optical interconnects  
- Wafer-scale fabrics  
- High-bandwidth memory clusters  

### ğŸ”¹ 6. Software and Parallelism  
Software becomes the biggest bottleneck without:

- Advanced compilers  
- Automatic parallelization  
- AI-driven task scheduling  
- Memory-centric programming models  

---

## ğŸ¯ The Road Ahead

If innovation continues across **materials â†’ devices â†’ packaging â†’ architecture â†’ software**, then:

> ğŸŸ¢ **Zetta-scale supercomputing is achievable within the next 10â€“12 years.**

This will unlock:

- Real-time global weather prediction  
- Human-scale biological simulation  
- Ultra-intelligent AI systems  
- Large-scale scientific discoveries  
- Nation-level cybersecurity analysis  
- Breakthroughs in climate, energy, and material science  

---

## ğŸ“Œ Conclusion

The journey to Zetta-scale computing isnâ€™t merely a chase for higher FLOPS.  
It is a challenge to deliver **more compute per watt** while overcoming the thermal, architectural, and economic limits of current technology.

### The transformation requires innovations across every layer:
> **Device â†’ Circuit â†’ Architecture â†’ System â†’ Application**

Your exploration of **FinFET-based circuit design and characterization** in this workshop is the foundation for understanding how the next generation of computing will be built â€” one transistor at a time.

---

## ğŸ“ 50 Years of Microprocessor Trend Data

For more than half a century, semiconductor progress has been governed by **Mooreâ€™s Law** â€” the observation that the number of transistors on an integrated circuit doubles roughly every **18â€“24 months**.  
This single trend has shaped all of modern computing, enabling continuous improvement in:

- ğŸ§  **CPU Single-thread performance** â€“ how fast a single core can execute instructions  
- âš¡ **Clock frequency** â€“ how many cycles per second the CPU operates  
- ğŸ”¥ **Compute per Watt** â€“ performance efficiency  
- ğŸ§© **Core count scaling** â€“ parallelism within a chip  

The chart below illustrates how these metrics evolved over *50+ years*, revealing the transition from frequency-driven scaling to today's era of parallel, power-aware architectures.

![50 Years of Microprocessor Trend Data](./images/50year_trend.png)

---

## ğŸ” What the Graph Tells Us â€” Deeper Interpretation

| Trend | Observation | Impact | What It Really Means |
|------|------------|--------|----------------------|
| ğŸŸ§ **Transistor Count** | Continues exponential rise | Enables more compute per die | Physical scaling (smaller transistors) continues, but at higher cost and complexity |
| ğŸ”µ **Single-thread Performance** | Growth slows significantly | Cannot rely on frequency boosts | Architectural innovation (out-of-order, speculation, caching) hits diminishing returns |
| ğŸŸ© **Frequency Scaling** | Plateaus around 2005 | â€œPower Wallâ€ stops GHz race | Increasing frequency drastically increases heat â†’ chips throttle |
| ğŸ”´ **Typical CPU Power** | Carefully capped (<200W for CPUs) | Avoids thermal failure and expensive packaging | Industry prioritizes energy efficiency over raw speed |
| âš« **Core Count** | Rapid rise after ~2005 | Multicore becomes default scaling method | Parallel workloads become necessary for performance gains |

---

## ğŸ“± The Mobile Revolution â€” A Turning Point in Computing History

In **2007**, something remarkable happened:  
The iPhone launched, marking the beginning of the **Mobile Computing Era**.

This shift redefined semiconductor priorities globally:

### Why Mobile Changed Everything
- Smartphones have **no fans**, so heat must be strictly limited.  
- They depend on **tiny batteries**, so power efficiency is critical.  
- They require **compact, energy-efficient silicon** to fit within slim designs.  

### The Industry Pivot
Before smartphones:
> Performance = Higher GHz

After smartphones:
> Performance = **More work per milliwatt**  
> (Performance per Watt becomes the new metric)

This forced chip designers to rethink everything:

- Smaller, more efficient transistors  
- big.LITTLE architectures  
- Low-power design methodologies  
- Specialized accelerators (ISP, GPU, NPU)  
- DVFS (Dynamic Voltage and Frequency Scaling)  

Mobile innovation didnâ€™t just transform phones â€” it reshaped **desktop, server, and datacenter processor design**, too.

---

## âš ï¸ Why Mooreâ€™s Law Still Matters (and Why Itâ€™s Harder Than Ever)

Achieving the next frontier â€” **Zetta-Scale Computing (10Â²Â¹ FLOPS)** â€” requires exponential improvements in compute density and efficiency.

But challenges are growing:

### âš¡ Physical Limits  
- Transistors are approaching atomic dimensions  
- Quantum tunneling increases leakage  
- Lithography becomes extremely expensive (EUV, HNL, multi-patterning)  

### ğŸ”¥ Thermal Limits  
- Smaller transistors = more heat density  
- Faster clocks = disproportionate power increase  

### ğŸ’¸ Economic Limits  
- Each new node costs billions  
- Only a few companies can afford leading-edge manufacturing  

Despite these challenges, **we must keep scaling**, even if Mooreâ€™s Law doesnâ€™t continue in its traditional form.

---

## ğŸ§  From Chips â†’ Systems: The New Era of Scaling

Exponential growth no longer comes from shrinking transistors alone.  
It now requires a *multi-level engineering effort*:

| Level | Innovation Required | Why It Matters |
|------|----------------------|----------------|
| **Transistor** | FinFET â†’ GAAFET â†’ 2D materials â†’ CNT-FETs | Lower leakage, higher electrostatic control |
| **Circuit** | 3D stacking, reduced parasitics, lower Vdd | Enables denser and faster logic |
| **Architecture** | Heterogeneous compute (CPU/GPU/NPU), vector engines | Optimizes for diverse workloads (AI, graphics, simulation) |
| **System** | Chiplets, advanced packaging, liquid cooling | Overcomes thermal and interconnect bottlenecks |
| **Software** | Parallel programming, memory-centric models, AI compilers | Unlocks hardware performance that would otherwise remain unused |

Scaling is now a **full-stack challenge** â€” from atoms to applications.

---

## ğŸ¯ Conclusion â€” The Future Depends on New Scaling Paradigms

> **Zetta-Scale computing (10Â²Â¹ FLOPS)** will only be possible if Mooreâ€™s Law continues through *non-classical* scaling.

This means:
- New transistor architectures (FinFET â†’ GAAFET â†’ CFET)  
- New materials (2D semiconductors, carbon nanotubes)  
- 3D chip stacking and advanced packaging  
- Energy-efficient accelerators  
- System-level co-optimization  
- AI-driven compiler and runtime innovation  

In short:

> **Mooreâ€™s Law is not dead â€” it is transforming.**  
> And this transformation will define the next era of supercomputing.

---

# CMOS Evolution and Next-Gen Device Candidates ğŸš€  
### A Timeline From Planar Transistors to 1nm and Beyond

Modern semiconductor scaling is a story of **constant reinvention**.  
What started as simple geometry scaling has evolved into a rich intersection of:

- ğŸ“¡ Advanced lithography  
- ğŸ§ª Novel materials  
- ğŸ§± New device architectures  
- ğŸ”„ Designâ€“technology co-optimization (DTCO)  
- ğŸ—ï¸ System-level scaling (STCO)

As transistors shrink toward atomic dimensions, traditional scaling breaks down â€” forcing the industry to introduce **new pillars of innovation** at key technological inflection points.

---

## ğŸ•“ Timeline of CMOS Evolution (Past â†’ Present â†’ Future)

Each decade brought a new fundamental challenge â€” and a breakthrough to overcome it.

| Year / Node | Major Innovation Pillar | Technology Introduced | Why This Was Needed |
|------------|-----------------------|----------------------|---------------------|
| **1990s â€” 130nm â†’ 90nm** | Channel engineering | **Strain-Silicon** | Boost carrier mobility as physical shrinking slowed |
| **2007 â€” 45nm** | Gate Stack revolution | **High-k + Metal Gate (HKMG)** | Reduce gate leakage caused by ultra-thin SiOâ‚‚ |
| **2012 â€” 22nm** | Transistor Architecture | **FinFET becomes mass-commercial** | Improve electrostatic control as planar MOSFETs hit limits |
| **2019 â€” 7nm EUV era** | Patterning breakthrough | **Extreme UV lithography (EUV)** | Enable further scaling without insane multi-patterning |
| **2022 â€” 3nm** | Channel & structure innovation | **SiGe channels, GAAFET R&D** | Prepare for gate-all-around control at 2nm nodes |
| **2025â€“2027 â€” 2nm roadmap** | Interconnect scaling | **Ru/Co wires, backside power delivery (BSPDN)** | Reduce RC delay & power routing bottlenecks |
| **Beyond 2028 â€” sub-1.8nm** | Novel channel materials | **2D materials (MoSâ‚‚, WSâ‚‚)** | Combat short-channel effects when silicon fails |
| **2030+ â€” 1nm target** | Vertical integration | **Vertical FETs, 3D stacked devices, chiplets** | Move beyond single-die scaling limits |

---

## ğŸ”º Scaling Drivers Through the Decades (What Changed and Why)

### ğŸ“ 1990â€“2010 â†’ **Geometry + Strain Scaling**
- Shrink gate lengths
- Introduce strain engineering  
- Improve dopant profiles  
â¡ Pure geometry scaling still viable.

### ğŸ“ 2010â€“2020 â†’ **Architecture Scaling (FinFET Era)**
- Planar MOSFET â†’ 3D FinFET  
- Better electrostatic control  
- Reduced leakage  
â¡ Architecture replaces geometry as primary driver.

### ğŸ“ 2020â€“2030 â†’ **Material + Lithography Era**
- EUV replaces complex multi-patterning  
- New interconnect metals (Ru, Co)  
- SiGe and nanosheet channels  
â¡ Materials enable continued density gains.

### ğŸ“ 2030+ â†’ **3D + System Scaling (STCO Era)**
- Chiplets  
- 3D die stacking  
- Backside power delivery  
â¡ Performance now comes from *systems*, not transistors.

> ğŸ“Œ The industry moved from **simple shrinking** â†’ **clever engineering** â†’ **system-wide optimization**.

---

## ğŸ”¬ The Five Scaling Pillars â€” Evolution of Each

Scaling is no longer one-dimensional. It relies on **five interdependent pillars**, each evolving at its own pace.

| Pillar | When It Started | Current Status (2025) | Next Evolution |
|-------|----------------|-----------------------|----------------|
| **Patterning** | ArF @ 248nm (90nm era) | EUV @ 7nm | High-NA EUV â†’ sub-1nm |
| **Gate Stack** | HKMG @ 45nm | Lower leakage & better reliability | Negative Capacitance FETs (NC-FETs), Ferroelectric materials |
| **Channel Material** | SiGe R&D @ 22nm | Nanosheet & SiGe channels | 2D semiconductors (MoSâ‚‚, WSâ‚‚), CNT-FETs |
| **Interconnects** | Cu adoption @ 180nm | Ru/Co + backside PDN | Topological semimetals & 3D interconnect fabrics |
| **Device Structure** | Planar â†’ FinFET @ 22nm | GAAFET/nanosheet @ 3nm | 3DS-FET, Vertical FETs (VFET), Stacked CFETs |

This marks the transition from **device-only scaling** â†’ **materials + system co-scaling**.

---

## ğŸ§© DTCO + STCO: The New Form of Mooreâ€™s Law

As physics limits intensify, the industry now depends on **co-optimization**, not just shrinking.

### ğŸ§  DTCO â€” *Designâ€“Technology Co-Optimization*
- Began around **7nm nodes**
- Co-designing:
  - Standard cell libraries  
  - Routing tracks  
  - Transistor geometry  
- Ensures PPA (Performanceâ€“Powerâ€“Area) gains even when pure scaling slows.

### ğŸ—ï¸ STCO â€” *Systemâ€“Technology Co-Optimization*
- Emerged with **3nm and chiplet architectures**
- Co-designing:
  - Multiple dies  
  - Packaging  
  - Interposers & stacked memory  
- Treats the **entire package as the new â€œscaling unitâ€**.

> ğŸ“Œ Future scaling will be defined not by â€œtransistor count per chipâ€ but by **performance per package** and **compute per watt per system**.

---

## ğŸ¯ Scaling Toward Zetta-Scale Computing (10Â²Â¹ FLOPS)

To reach Zetta-scale performance, we must solve challenges at each level:

### ğŸ”½ Device-Level Limits
- FinFET scaling saturates ~5nm  
- GAAFET/nanosheet extends to ~2nm  
- Beyond that â†’ **silicon bandgap + electrostatic limits**

### ğŸ”½ Material Limits
- Silicon mobility insufficient for <1.5nm channels  
- Need:
  - 2D materials (MoSâ‚‚, WSâ‚‚)  
  - Carbon nanotubes  
  - Topological semimetals  

### ğŸ”½ Interconnect & Power Limits
- RC delay dominates performance  
- Backside power delivery becomes essential  
- Ru/Co replace Cu due to electromigration limits

### ğŸ”½ System-Level Limits
- Monolithic die becomes impractical  
- Chiplet-based HPC and AI accelerators become necessary  
- 3D stacking enables â€œvertical Mooreâ€™s Lawâ€

---

## ğŸ“Œ Summary: How Scaling Has Evolved Over Time

| Era | Primary Scaling Method | Example Technologies |
|-----|------------------------|----------------------|
| **1990â€“2010** (Golden Mooreâ€™s Law) | **Physical shrinking** | Planar â†’ Strain â†’ HKMG |
| **2012â€“2022** (Electrostatic Era) | **3D device structures** | FinFET |
| **2022â€“2028** (Materialâ€“Interconnect Era) | **Novel materials + EUV** | Ru, Co, SiGe, EUV |
| **2028+** (System Scaling Era) | **Chiplets + advanced packaging** | STCO, 3D ICs |

> ğŸš€ The journey toward **1nm** and **ZettaFLOPS** requires a synergy of **materials science, device engineering, architecture, and system design** â€” all scaling together.

---

## ğŸ“˜ Introduction to FinFETs

As transistor dimensions scaled down into the nanometer regime, traditional planar MOSFETs began to suffer from performance degradation, increased leakage, and loss of electrostatic control. To overcome these challenges, industry shifted toward advanced multigate transistor architectures such as **FinFETs** and **Gate-All-Around (GAA)** devices.
The images below illustrate this evolution, explain why FinFETs are necessary, and show their impact on circuit performance.

### ğŸ”¹ 1. Transistor Evolution: From Planar to FinFET to Gate-All-Around
The first image highlights the progression of transistor architectures:

#### Planar MOSFET
- Gate sits on top of a flat silicon channel.
- Limited gate control as channel becomes thinner.
- Suffered from short-channel effects at advanced nodes (<40nm).

#### FinFET (Tri-Gate) â€” Introduced around 2011
- Silicon channel shaped like a vertical fin.
- Gate wraps around **three sides** of the fin.
- Provides much stronger electrostatic control over the channel.
- Reduces leakage and improves switching speed.

#### Gate-All-Around (GAAFET) â€” Expected around 2025
- Uses stacked nanosheets or nanowires.
- Gate completely surrounds the channel from all sides.
- Allows higher drive current in a smaller footprint.
- Considered the successor to FinFETs for sub-3nm nodes.

This architectural evolution allows more drive current per device footprint, enabling both performance and density scaling.


### ğŸ”¹ 2. Why FinFETs? Improved Gate Control & Reduced Leakage
The second image explains the motivation for FinFET and GAA technologies.
#### Planar MOSFET Issues
- Weak gate control over the channel.
- Significant **sub-channel leakage**, especially at short channel lengths.
- Poor scalability beyond 32nm.
#### FinFET Improvements
- Gate wraps around the fin, improving control.
- Reduced subthreshold leakage.
- Better short-channel behavior.
#### Gate-All-Around Advantages
- Gate surrounds the channel fully, maximizing control.
- Reduced drain-induced effects.
- Lower subthreshold slope (S.S.), improving low-power operation.
A comparison plot shows how FinFETs achieve a steeper subthreshold slope and lower leakage versus planar devices.

### ğŸ”¹ 3. Impact on Circuit Performance
The third image demonstrates how FinFETs improve circuit-level characteristics.
#### Lower Off-Current
- FinFETs reduce leakage current dramatically.
- Lower static power consumption.
#### Higher Drive Current
- At the same gate voltage, FinFETs achieve higher on-current than planar MOSFETs.
- This leads to faster switching and better performance.
#### Overall Performance Benefits
- Higher Ion / Ioff ratio
- Improved switching speed
- Reduced leakage power
- Better scalability for advanced nodes
These characteristics are why FinFETs became the industry standard from 22nm down to 5nm, and why GAA devices will drive technology below 3nm.

---
